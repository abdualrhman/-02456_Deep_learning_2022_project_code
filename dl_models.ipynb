{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "import seaborn as sns\n",
    "from DataModel import DataModel, TensorDataSet, TensorTabDataSet\n",
    "from sklearn import metrics\n",
    "from torch.utils.data import DataLoader\n",
    "from datetime import datetime\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tab_transformer_pytorch import TabTransformer, FTTransformer\n",
    "# from tensorboard import SummaryWriter\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "working_directory = os.getcwd()\n",
    "file_path = working_directory + '/final_dataset.pickle'\n",
    "with open(file_path, 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DL models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FT-Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_encoding_type = 'label'\n",
    "cols_to_ohe = ['priority', 'deck_on_vessel', 'is_reefer', 'is_hazardous', 'unitype_id']\n",
    "dm = DataModel(encoding_type=data_encoding_type, cols_to_ohe=cols_to_ohe)\n",
    "df = dm.get_df()\n",
    "X,y = dm.get_inputs_targets()\n",
    "def get_cols_unique_num():\n",
    "    uniques = []\n",
    "    for ind, i in enumerate(X.columns):\n",
    "        if ind == 0 or ind == 1: continue\n",
    "        uniques.append(len(X[i].unique()))\n",
    "    return tuple(uniques)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "tr_tab_ds = TensorTabDataSet(data_type='train',normalize_num=False, encoding_type=data_encoding_type, cols_to_ohe=cols_to_ohe)\n",
    "vl_tab_ds = TensorTabDataSet(data_type='valid', normalize_num=False,encoding_type=data_encoding_type, cols_to_ohe=cols_to_ohe)\n",
    "train_tab_loader =  DataLoader(tr_tab_ds, batch_size=batch_size)\n",
    "valid_tab_loader =  DataLoader(vl_tab_ds, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FTTransformer(\n",
       "  (categorical_embeds): Embedding(590, 64)\n",
       "  (numerical_embedder): NumericalEmbedder()\n",
       "  (transformer): Transformer(\n",
       "    (layers): ModuleList(\n",
       "      (0): ModuleList(\n",
       "        (0): Attention(\n",
       "          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "          (to_qkv): Linear(in_features=64, out_features=384, bias=False)\n",
       "          (to_out): Linear(in_features=128, out_features=64, bias=False)\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): Linear(in_features=64, out_features=512, bias=True)\n",
       "          (2): GEGLU()\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "          (4): Linear(in_features=256, out_features=64, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (1): ModuleList(\n",
       "        (0): Attention(\n",
       "          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "          (to_qkv): Linear(in_features=64, out_features=384, bias=False)\n",
       "          (to_out): Linear(in_features=128, out_features=64, bias=False)\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): Linear(in_features=64, out_features=512, bias=True)\n",
       "          (2): GEGLU()\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "          (4): Linear(in_features=256, out_features=64, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (2): ModuleList(\n",
       "        (0): Attention(\n",
       "          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "          (to_qkv): Linear(in_features=64, out_features=384, bias=False)\n",
       "          (to_out): Linear(in_features=128, out_features=64, bias=False)\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): Linear(in_features=64, out_features=512, bias=True)\n",
       "          (2): GEGLU()\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "          (4): Linear(in_features=256, out_features=64, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (3): ModuleList(\n",
       "        (0): Attention(\n",
       "          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "          (to_qkv): Linear(in_features=64, out_features=384, bias=False)\n",
       "          (to_out): Linear(in_features=128, out_features=64, bias=False)\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): Linear(in_features=64, out_features=512, bias=True)\n",
       "          (2): GEGLU()\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "          (4): Linear(in_features=256, out_features=64, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (4): ModuleList(\n",
       "        (0): Attention(\n",
       "          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "          (to_qkv): Linear(in_features=64, out_features=384, bias=False)\n",
       "          (to_out): Linear(in_features=128, out_features=64, bias=False)\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): Linear(in_features=64, out_features=512, bias=True)\n",
       "          (2): GEGLU()\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "          (4): Linear(in_features=256, out_features=64, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (5): ModuleList(\n",
       "        (0): Attention(\n",
       "          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "          (to_qkv): Linear(in_features=64, out_features=384, bias=False)\n",
       "          (to_out): Linear(in_features=128, out_features=64, bias=False)\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): Linear(in_features=64, out_features=512, bias=True)\n",
       "          (2): GEGLU()\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "          (4): Linear(in_features=256, out_features=64, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (to_logits): Sequential(\n",
       "    (0): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=64, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_cat_num_tuple = get_cols_unique_num()\n",
    "\n",
    "model = FTTransformer(\n",
    "    categories = unique_cat_num_tuple,\n",
    "    num_continuous = 2,          \n",
    "    dim = 64,                     \n",
    "    dim_out = 1,                  \n",
    "    depth = 6,                    \n",
    "    heads = 8,                    \n",
    "    attn_dropout = 0.2,           \n",
    "    ff_dropout = 0.1              \n",
    ")\n",
    "device = torch.device('cpu')  # use cuda or cpu\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "learning_rate = 0.0001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)  \n",
    "def accuracy(target, pred):\n",
    "    return metrics.r2_score(target.detach().cpu().numpy(), pred.detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_accuracies=[]\n",
    "valid_accuracies = []\n",
    "def train_one_epoch(epoch_index, tb_writer):\n",
    "    running_loss = 0.\n",
    "    last_loss = 0.\n",
    "\n",
    "    for i, data in enumerate(train_tab_loader):\n",
    "        train_accuracies_batches = []\n",
    "        # Every data instance is an input + label pair\n",
    "        inputs, targets = data\n",
    "        cat_inputs, num_inputs = inputs[0], inputs[1]\n",
    "        targets = targets.type(torch.FloatTensor)\n",
    "        targets = targets.unsqueeze(dim=1)\n",
    "\n",
    "        # Zero your gradients for every batch!\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Make predictions for this batch\n",
    "        outputs = model(cat_inputs, num_inputs)\n",
    "\n",
    "        # Compute the loss and its gradients\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        # predictions = outputs\n",
    "\n",
    "        # Adjust learning weights\n",
    "        optimizer.step()\n",
    "        train_accuracies_batches.append(accuracy(targets, outputs))\n",
    "\n",
    "        # Gather data and report\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        if i % 100 == 99:\n",
    "            last_loss = running_loss / 100 # loss per batch\n",
    "            print('  batch {} loss: {}'.format(i + 1, last_loss))\n",
    "            tb_x = epoch_index * len(train_tab_loader) + i + 1\n",
    "            tb_writer.add_scalar('Loss/train', last_loss, tb_x)\n",
    "            running_loss = 0.\n",
    "    train_accuracies.append(np.mean(train_accuracies_batches))\n",
    "\n",
    "    return last_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1:\n",
      "LOSS train 0.0 valid 17539.291015625\n",
      "EPOCH 2:\n",
      "LOSS train 0.0 valid 17460.220703125\n",
      "EPOCH 3:\n",
      "LOSS train 0.0 valid 17395.314453125\n",
      "EPOCH 4:\n",
      "LOSS train 0.0 valid 17342.02734375\n",
      "EPOCH 5:\n",
      "LOSS train 0.0 valid 17303.923828125\n",
      "EPOCH 6:\n",
      "LOSS train 0.0 valid 17275.3828125\n",
      "EPOCH 7:\n",
      "LOSS train 0.0 valid 17254.474609375\n",
      "EPOCH 8:\n",
      "LOSS train 0.0 valid 17240.169921875\n",
      "EPOCH 9:\n",
      "LOSS train 0.0 valid 17230.208984375\n",
      "EPOCH 10:\n",
      "LOSS train 0.0 valid 17223.578125\n",
      "EPOCH 11:\n",
      "LOSS train 0.0 valid 17219.171875\n",
      "EPOCH 12:\n",
      "LOSS train 0.0 valid 17215.9609375\n",
      "EPOCH 13:\n",
      "LOSS train 0.0 valid 17213.060546875\n",
      "EPOCH 14:\n",
      "LOSS train 0.0 valid 17210.021484375\n",
      "EPOCH 15:\n",
      "LOSS train 0.0 valid 17206.80859375\n",
      "EPOCH 16:\n",
      "LOSS train 0.0 valid 17203.548828125\n",
      "EPOCH 17:\n",
      "LOSS train 0.0 valid 17200.390625\n",
      "EPOCH 18:\n",
      "LOSS train 0.0 valid 17197.330078125\n",
      "EPOCH 19:\n",
      "LOSS train 0.0 valid 17194.341796875\n",
      "EPOCH 20:\n",
      "LOSS train 0.0 valid 17191.384765625\n",
      "EPOCH 21:\n",
      "LOSS train 0.0 valid 17188.44140625\n",
      "EPOCH 22:\n",
      "LOSS train 0.0 valid 17185.49609375\n",
      "EPOCH 23:\n",
      "LOSS train 0.0 valid 17182.55859375\n",
      "EPOCH 24:\n",
      "LOSS train 0.0 valid 17179.625\n",
      "EPOCH 25:\n",
      "LOSS train 0.0 valid 17176.6875\n",
      "EPOCH 26:\n",
      "LOSS train 0.0 valid 17173.75\n",
      "EPOCH 27:\n",
      "LOSS train 0.0 valid 17170.814453125\n",
      "EPOCH 28:\n",
      "LOSS train 0.0 valid 17167.88671875\n",
      "EPOCH 29:\n",
      "LOSS train 0.0 valid 17164.970703125\n",
      "EPOCH 30:\n",
      "LOSS train 0.0 valid 17162.04296875\n",
      "EPOCH 31:\n",
      "LOSS train 0.0 valid 17159.11328125\n",
      "EPOCH 32:\n",
      "LOSS train 0.0 valid 17156.19140625\n",
      "EPOCH 33:\n",
      "LOSS train 0.0 valid 17153.271484375\n",
      "EPOCH 34:\n",
      "LOSS train 0.0 valid 17150.341796875\n",
      "EPOCH 35:\n",
      "LOSS train 0.0 valid 17147.408203125\n",
      "EPOCH 36:\n",
      "LOSS train 0.0 valid 17144.4765625\n",
      "EPOCH 37:\n",
      "LOSS train 0.0 valid 17141.537109375\n",
      "EPOCH 38:\n",
      "LOSS train 0.0 valid 17138.603515625\n",
      "EPOCH 39:\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/dw/gf2x93p50fdd3796cx4grnqw0000gn/T/ipykernel_75761/1980646416.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m# Make sure gradient tracking is on, and do a pass over the data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mavg_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_one_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_number\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwriter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;31m# We don't need gradients on to do reporting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/dw/gf2x93p50fdd3796cx4grnqw0000gn/T/ipykernel_75761/675593684.py\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[0;34m(epoch_index, tb_writer)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;31m# Make predictions for this batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcat_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;31m# Compute the loss and its gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/tab_transformer_pytorch/ft_transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x_categ, x_numer)\u001b[0m\n\u001b[1;32m    209\u001b[0m         \u001b[0;31m# attend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;31m# get cls token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/tab_transformer_pytorch/ft_transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mattn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mff\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mff\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/tab_transformer_pytorch/ft_transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_qkv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m         \u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mrearrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'b n (h d) -> b h n d'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0mq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mq\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "writer = SummaryWriter('runs/TabData(dropout){}'.format(timestamp))\n",
    "epoch_number = 0\n",
    "valid_preds = []\n",
    "valid_targs = []\n",
    "EPOCHS = 65\n",
    "\n",
    "best_vloss = 1_000_000.\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print('EPOCH {}:'.format(epoch_number + 1))\n",
    "\n",
    "    # Make sure gradient tracking is on, and do a pass over the data\n",
    "    model.train(True)\n",
    "    avg_loss = train_one_epoch(epoch_number, writer)\n",
    "\n",
    "    # We don't need gradients on to do reporting\n",
    "    model.train(False)\n",
    "\n",
    "    running_vloss = 0.0\n",
    "    for i, vdata in enumerate(valid_tab_loader):\n",
    "        valid_accuracies_batches = []\n",
    "        vinputs, vtargets = vdata\n",
    "        vcat_inputs, vnum_inputs = vinputs[0], vinputs[1]\n",
    "        vtargets = vtargets.type(torch.FloatTensor)\n",
    "        vtargets = vtargets.unsqueeze(dim=1)\n",
    "        voutputs = model(vcat_inputs, vnum_inputs)\n",
    "        valid_preds.append(voutputs.detach().cpu().numpy())\n",
    "        valid_targs.append(vtargets.detach().cpu().numpy())\n",
    "        vloss = criterion(voutputs, vtargets)\n",
    "        running_vloss += vloss\n",
    "        valid_accuracies_batches.append(accuracy(vtargets, voutputs))\n",
    "\n",
    "    valid_accuracies.append(np.mean(valid_accuracies_batches))\n",
    "    avg_vloss = running_vloss / (i + 1)\n",
    "    print('LOSS train {} valid {}'.format(avg_loss, avg_vloss))\n",
    "\n",
    "    # Log the running loss averaged per batch\n",
    "    # for both training and validation\n",
    "    writer.add_scalars('Training vs. Validation Loss',\n",
    "                    { 'Training' : avg_loss, 'Validation' : avg_vloss },\n",
    "                    epoch_number + 1)\n",
    "    writer.flush()\n",
    "\n",
    "    # Track best performance, and save the model's state\n",
    "    if avg_vloss < best_vloss:\n",
    "        best_vloss = avg_vloss\n",
    "        model_path = 'model_{}_{}'.format(timestamp, epoch_number)\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "\n",
    "    epoch_number += 1\n",
    "print(\"=== Done ===\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_ff_ds = TensorDataSet(data_type='train',normalize_num=False, encoding_type=data_encoding_type, cols_to_ohe=cols_to_ohe)\n",
    "vl_ff_ds = TensorDataSet(data_type='valid', normalize_num=False,encoding_type=data_encoding_type, cols_to_ohe=cols_to_ohe)\n",
    "\n",
    "train_ff_loader =  DataLoader(tr_ff_ds, batch_size=batch_size)\n",
    "valid_ff_loader =  DataLoader(vl_ff_ds, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class FFModel(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(FFModel, self).__init__()  \n",
    "        self.input_size =input_size\n",
    "        self.l1=nn.Linear(self.input_size, self.input_size*2)\n",
    "        self.l2=nn.Linear(self.input_size*2, self.input_size*10)\n",
    "        self.l3=nn.Linear(self.input_size*10, self.input_size*20)\n",
    "        self.l4=nn.Linear(self.input_size*20, self.input_size*50)\n",
    "        self.l5=nn.Linear(self.input_size*50, self.input_size*55)\n",
    "        self.l6=nn.Linear(self.input_size*55, 1)\n",
    "        self.relu=nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.30)\n",
    "    def forward(self, x):\n",
    "        out=self.l1(x)\n",
    "        out=self.relu(out)\n",
    "        self.dropout(out)\n",
    "        out=self.l2(out)\n",
    "        out=self.relu(out)\n",
    "        self.dropout(out)\n",
    "        out=self.l3(out)\n",
    "        out=self.relu(out)\n",
    "        self.dropout(out)\n",
    "        out=self.l4(out)\n",
    "        out=self.relu(out)\n",
    "        self.dropout(out)\n",
    "        out=self.l5(out)\n",
    "        out=self.relu(out)\n",
    "        out=self.l6(out)\n",
    "        return out\n",
    "data_encoding_type = 'label'\n",
    "cols_to_ohe = ['priority', 'deck_on_vessel', 'is_reefer', 'is_hazardous', 'unitype_id']\n",
    "dm = DataModel(encoding_type=data_encoding_type, normalize_num=False, cols_to_ohe=cols_to_ohe)\n",
    "df = dm.get_df()\n",
    "input_size = dm.get_inputs_targets()[0].shape[1]\n",
    "model2 = FFModel(input_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_accuracies=[]\n",
    "valid_accuracies = []\n",
    "def train_one_epochFFM(epoch_index, tb_writer):\n",
    "    running_loss = 0.\n",
    "    last_loss = 0.\n",
    "\n",
    "    # Here, we use enumerate(training_loader) instead of\n",
    "    # iter(training_loader) so that we can track the batch\n",
    "    # index and do some intra-epoch reporting\n",
    "    for i, data in enumerate(train_ff_loader):\n",
    "        train_accuracies_batches = []\n",
    "        # Every data instance is an input + label pair\n",
    "        # cat_inputs, num_inputs = inputs[0], inputs[1]\n",
    "        inputs, targets = data\n",
    "        targets = targets.type(torch.FloatTensor)\n",
    "        targets = targets.unsqueeze(dim=1)\n",
    "        # Zero your gradients for every batch!\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Make predictions for this batch\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Compute the loss and its gradients\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "\n",
    "        # Adjust learning weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Gather data and report\n",
    "        running_loss += loss.item()\n",
    "        train_accuracies_batches.append(accuracy(targets, outputs))\n",
    "\n",
    "        if i % 100 == 99:\n",
    "            last_loss = running_loss / 100 # loss per batch\n",
    "            print('  batch {} loss: {}'.format(i + 1, last_loss))\n",
    "            tb_x = epoch_index * len(train_ff_loader) + i + 1\n",
    "            tb_writer.add_scalar('Loss/train', last_loss, tb_x)\n",
    "            running_loss = 0.\n",
    "    train_accuracies.append(np.mean(train_accuracies_batches))\n",
    "\n",
    "    return last_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "writer = SummaryWriter('runs/FNN{}'.format(timestamp))\n",
    "epoch_number = 0\n",
    "\n",
    "EPOCHS = 65\n",
    "\n",
    "best_vloss = 1_000_000.\n",
    "train_accuracies = []\n",
    "valid_accuracies = []\n",
    "for epoch in range(EPOCHS):\n",
    "    print('EPOCH {}:'.format(epoch_number + 1))\n",
    "\n",
    "    # Make sure gradient tracking is on, and do a pass over the data\n",
    "    model.train(True)\n",
    "    avg_loss = train_one_epochFFM(epoch_number, writer)\n",
    "\n",
    "    # We don't need gradients on to do reporting\n",
    "    model.train(False)\n",
    "\n",
    "    running_vloss = 0.0\n",
    "    for i, vdata in enumerate(valid_ff_loader):\n",
    "        valid_accuracies_batches = []\n",
    "        vinputs, vtargets = vdata\n",
    "        vtargets = vtargets.type(torch.FloatTensor)\n",
    "        vtargets = vtargets.unsqueeze(dim=1)\n",
    "        outputs = model(vinputs)\n",
    "        vloss = criterion(outputs, vtargets)\n",
    "        running_vloss += vloss\n",
    "        valid_accuracies_batches.append(accuracy(vtargets, outputs))\n",
    "\n",
    "    valid_accuracies.append(np.mean(valid_accuracies_batches))\n",
    "    avg_vloss = running_vloss / (i + 1)\n",
    "    print('LOSS train {} valid {}'.format(avg_loss, avg_vloss))\n",
    "\n",
    "    # Log the running loss averaged per batch\n",
    "    # for both training and validation\n",
    "    writer.add_scalars('Training vs. Validation Loss',\n",
    "                    { 'Training' : avg_loss, 'Validation' : avg_vloss },\n",
    "                    epoch_number + 1)\n",
    "    writer.flush()\n",
    "\n",
    "    # Track best performance, and save the model's state\n",
    "    if avg_vloss < best_vloss:\n",
    "        best_vloss = avg_vloss\n",
    "        model_path = 'model_{}_{}'.format(timestamp, epoch_number)\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "\n",
    "    epoch_number += 1\n",
    "epoch = np.arange(len(train_accuracies))\n",
    "plt.figure()\n",
    "plt.plot(epoch, train_accuracies, 'r', epoch, valid_accuracies, 'b')\n",
    "plt.legend(['Train Accucary','Validation Accuracy'])\n",
    "plt.xlabel('Updates'), plt.ylabel('Acc')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d685bfd7114377445cb2c23cc6bfca7f1f2544957139cb4a27ccab868339e3e1"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
