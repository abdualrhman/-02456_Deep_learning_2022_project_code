{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "import seaborn as sns\n",
    "from DataModel import DataModel, TensorDataSet, TensorTabDataSet\n",
    "from sklearn import metrics\n",
    "from torch.utils.data import DataLoader\n",
    "from datetime import datetime\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tab_transformer_pytorch import TabTransformer, FTTransformer\n",
    "# from tensorboard import SummaryWriter\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "working_directory = os.getcwd()\n",
    "file_path = working_directory + '/final_dataset.pickle'\n",
    "with open(file_path, 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is obtained fron the shiping and logistic company DFDS and is confidential. Therefor we only share a subset of the data, approx. 350 rows of the original 43890 rows. Thus, it's not possible to reproduce the results in the report from the following script. However, the following show the model implementaion and the training procces.\n",
    "\n",
    "The class `DataModel` handles the data processing. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DL models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FT-Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_encoding_type = 'label'\n",
    "cols_to_ohe = ['priority', 'deck_on_vessel', 'is_reefer', 'is_hazardous', 'unitype_id']\n",
    "dm = DataModel(encoding_type=data_encoding_type, cols_to_ohe=cols_to_ohe)\n",
    "df = dm.get_df()\n",
    "X,y = dm.get_inputs_targets()\n",
    "def get_cols_unique_num():\n",
    "    uniques = []\n",
    "    for ind, i in enumerate(X.columns):\n",
    "        if ind == 0 or ind == 1: continue\n",
    "        uniques.append(len(X[i].unique()))\n",
    "    return tuple(uniques)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "tr_tab_ds = TensorTabDataSet(data_type='train',normalize_num=False, encoding_type=data_encoding_type, cols_to_ohe=cols_to_ohe)\n",
    "vl_tab_ds = TensorTabDataSet(data_type='valid', normalize_num=False,encoding_type=data_encoding_type, cols_to_ohe=cols_to_ohe)\n",
    "train_tab_loader =  DataLoader(tr_tab_ds, batch_size=batch_size)\n",
    "valid_tab_loader =  DataLoader(vl_tab_ds, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_cat_num_tuple = get_cols_unique_num()\n",
    "\n",
    "model = FTTransformer(\n",
    "    categories = unique_cat_num_tuple,\n",
    "    num_continuous = 2,          \n",
    "    dim = 64,                     \n",
    "    dim_out = 1,                  \n",
    "    depth = 6,                    \n",
    "    heads = 8,                    \n",
    "    attn_dropout = 0.2,           \n",
    "    ff_dropout = 0.1              \n",
    ")\n",
    "device = torch.device('cpu')  # use cuda or cpu\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "learning_rate = 0.0001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)  \n",
    "def accuracy(target, pred):\n",
    "    return metrics.r2_score(target.detach().cpu().numpy(), pred.detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_accuracies=[]\n",
    "valid_accuracies = []\n",
    "def train_one_epoch(epoch_index, tb_writer):\n",
    "    running_loss = 0.\n",
    "    last_loss = 0.\n",
    "\n",
    "    for i, data in enumerate(train_tab_loader):\n",
    "        train_accuracies_batches = []\n",
    "        # Every data instance is an input + label pair\n",
    "        inputs, targets = data\n",
    "        cat_inputs, num_inputs = inputs[0], inputs[1]\n",
    "        targets = targets.type(torch.FloatTensor)\n",
    "        targets = targets.unsqueeze(dim=1)\n",
    "\n",
    "        # Zero your gradients for every batch!\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Make predictions for this batch\n",
    "        outputs = model(cat_inputs, num_inputs)\n",
    "\n",
    "        # Compute the loss and its gradients\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        # predictions = outputs\n",
    "\n",
    "        # Adjust learning weights\n",
    "        optimizer.step()\n",
    "        train_accuracies_batches.append(accuracy(targets, outputs))\n",
    "\n",
    "        # Gather data and report\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        if i % 100 == 99:\n",
    "            last_loss = running_loss / 100 # loss per batch\n",
    "            print('  batch {} loss: {}'.format(i + 1, last_loss))\n",
    "            tb_x = epoch_index * len(train_tab_loader) + i + 1\n",
    "            tb_writer.add_scalar('Loss/train', last_loss, tb_x)\n",
    "            running_loss = 0.\n",
    "    train_accuracies.append(np.mean(train_accuracies_batches))\n",
    "\n",
    "    return last_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "writer = SummaryWriter('runs/TabData(dropout){}'.format(timestamp))\n",
    "epoch_number = 0\n",
    "valid_preds = []\n",
    "valid_targs = []\n",
    "EPOCHS = 65\n",
    "\n",
    "best_vloss = 1_000_000.\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print('EPOCH {}:'.format(epoch_number + 1))\n",
    "\n",
    "    # Make sure gradient tracking is on, and do a pass over the data\n",
    "    model.train(True)\n",
    "    avg_loss = train_one_epoch(epoch_number, writer)\n",
    "\n",
    "    # We don't need gradients on to do reporting\n",
    "    model.train(False)\n",
    "\n",
    "    running_vloss = 0.0\n",
    "    for i, vdata in enumerate(valid_tab_loader):\n",
    "        valid_accuracies_batches = []\n",
    "        vinputs, vtargets = vdata\n",
    "        vcat_inputs, vnum_inputs = vinputs[0], vinputs[1]\n",
    "        vtargets = vtargets.type(torch.FloatTensor)\n",
    "        vtargets = vtargets.unsqueeze(dim=1)\n",
    "        voutputs = model(vcat_inputs, vnum_inputs)\n",
    "        valid_preds.append(voutputs.detach().cpu().numpy())\n",
    "        valid_targs.append(vtargets.detach().cpu().numpy())\n",
    "        vloss = criterion(voutputs, vtargets)\n",
    "        running_vloss += vloss\n",
    "        valid_accuracies_batches.append(accuracy(vtargets, voutputs))\n",
    "\n",
    "    valid_accuracies.append(np.mean(valid_accuracies_batches))\n",
    "    avg_vloss = running_vloss / (i + 1)\n",
    "    print('LOSS train {} valid {}'.format(avg_loss, avg_vloss))\n",
    "\n",
    "    # Log the running loss averaged per batch\n",
    "    # for both training and validation\n",
    "    writer.add_scalars('Training vs. Validation Loss',\n",
    "                    { 'Training' : avg_loss, 'Validation' : avg_vloss },\n",
    "                    epoch_number + 1)\n",
    "    writer.flush()\n",
    "\n",
    "    # Track best performance, and save the model's state\n",
    "    if avg_vloss < best_vloss:\n",
    "        best_vloss = avg_vloss\n",
    "        model_path = 'model_{}_{}'.format(timestamp, epoch_number)\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "\n",
    "    epoch_number += 1\n",
    "print(\"=== Done ===\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = np.arange(len(train_accuracies))\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.plot(epoch, train_accuracies, 'r', epoch, valid_accuracies, 'b')\n",
    "plt.yticks([0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1])\n",
    "plt.legend(['Train Accucary','Validation Accuracy'])\n",
    "plt.xlabel('Updates'), plt.ylabel('Acc')\n",
    "print(\"train acc: \",np.mean(train_accuracies))\n",
    "print(\"validation acc: \", np.mean(valid_accuracies))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_ff_ds = TensorDataSet(data_type='train',normalize_num=False, encoding_type=data_encoding_type, cols_to_ohe=cols_to_ohe)\n",
    "vl_ff_ds = TensorDataSet(data_type='valid', normalize_num=False,encoding_type=data_encoding_type, cols_to_ohe=cols_to_ohe)\n",
    "\n",
    "train_ff_loader =  DataLoader(tr_ff_ds, batch_size=batch_size)\n",
    "valid_ff_loader =  DataLoader(vl_ff_ds, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class FFModel(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(FFModel, self).__init__()  \n",
    "        self.input_size =input_size\n",
    "        self.l1=nn.Linear(self.input_size, self.input_size*2)\n",
    "        self.l2=nn.Linear(self.input_size*2, self.input_size*10)\n",
    "        self.l3=nn.Linear(self.input_size*10, self.input_size*20)\n",
    "        self.l4=nn.Linear(self.input_size*20, self.input_size*50)\n",
    "        self.l5=nn.Linear(self.input_size*50, self.input_size*55)\n",
    "        self.l6=nn.Linear(self.input_size*55, 1)\n",
    "        self.relu=nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.30)\n",
    "    def forward(self, x):\n",
    "        out=self.l1(x)\n",
    "        out=self.relu(out)\n",
    "        self.dropout(out)\n",
    "        out=self.l2(out)\n",
    "        out=self.relu(out)\n",
    "        self.dropout(out)\n",
    "        out=self.l3(out)\n",
    "        out=self.relu(out)\n",
    "        self.dropout(out)\n",
    "        out=self.l4(out)\n",
    "        out=self.relu(out)\n",
    "        self.dropout(out)\n",
    "        out=self.l5(out)\n",
    "        out=self.relu(out)\n",
    "        out=self.l6(out)\n",
    "        return out\n",
    "# encode_type = ''\n",
    "# cols_to_label_encode=['place_on_board']\n",
    "data_encoding_type = 'label'\n",
    "cols_to_ohe = ['priority', 'deck_on_vessel', 'is_reefer', 'is_hazardous', 'unitype_id']\n",
    "dm = DataModel(encoding_type=data_encoding_type, normalize_num=False, cols_to_ohe=cols_to_ohe)\n",
    "df = dm.get_df()\n",
    "input_size = dm.get_inputs_targets()[0].shape[1]\n",
    "model = FFModel(input_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training loop "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By running the following script, the training loop will begin and for each epoch, model parameter will be saved. We select the model that preforms best on the validation set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_accuracies=[]\n",
    "valid_accuracies = []\n",
    "def train_one_epochFFM(epoch_index, tb_writer):\n",
    "    running_loss = 0.\n",
    "    last_loss = 0.\n",
    "\n",
    "    for i, data in enumerate(train_ff_loader):\n",
    "        train_accuracies_batches = []\n",
    "\n",
    "        inputs, targets = data\n",
    "        targets = targets.type(torch.FloatTensor)\n",
    "        targets = targets.unsqueeze(dim=1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Gather data and report\n",
    "        running_loss += loss.item()\n",
    "        train_accuracies_batches.append(accuracy(targets, outputs))\n",
    "\n",
    "        if i % 100 == 99:\n",
    "            last_loss = running_loss / 100 # loss per batch\n",
    "            print('  batch {} loss: {}'.format(i + 1, last_loss))\n",
    "            tb_x = epoch_index * len(train_ff_loader) + i + 1\n",
    "            tb_writer.add_scalar('Loss/train', last_loss, tb_x)\n",
    "            running_loss = 0.\n",
    "    train_accuracies.append(np.mean(train_accuracies_batches))\n",
    "\n",
    "    return last_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "writer = SummaryWriter('runs/FNN{}'.format(timestamp))\n",
    "epoch_number = 0\n",
    "\n",
    "EPOCHS = 65\n",
    "\n",
    "best_vloss = 1_000_000.\n",
    "train_accuracies = []\n",
    "valid_accuracies = []\n",
    "for epoch in range(EPOCHS):\n",
    "    print('EPOCH {}:'.format(epoch_number + 1))\n",
    "\n",
    "    # Make sure gradient tracking is on, and do a pass over the data\n",
    "    model.train(True)\n",
    "    avg_loss = train_one_epochFFM(epoch_number, writer)\n",
    "\n",
    "    # We don't need gradients on to do reporting\n",
    "    model.train(False)\n",
    "\n",
    "    running_vloss = 0.0\n",
    "    for i, vdata in enumerate(valid_ff_loader):\n",
    "        valid_accuracies_batches = []\n",
    "        vinputs, vtargets = vdata\n",
    "        vtargets = vtargets.type(torch.FloatTensor)\n",
    "        vtargets = vtargets.unsqueeze(dim=1)\n",
    "        outputs = model(vinputs)\n",
    "        vloss = criterion(outputs, vtargets)\n",
    "        running_vloss += vloss\n",
    "        valid_accuracies_batches.append(accuracy(vtargets, outputs))\n",
    "\n",
    "    valid_accuracies.append(np.mean(valid_accuracies_batches))\n",
    "    avg_vloss = running_vloss / (i + 1)\n",
    "    print('LOSS train {} valid {}'.format(avg_loss, avg_vloss))\n",
    "\n",
    "    # Log the running loss averaged per batch\n",
    "    # for both training and validation\n",
    "    writer.add_scalars('Training vs. Validation Loss',\n",
    "                    { 'Training' : avg_loss, 'Validation' : avg_vloss },\n",
    "                    epoch_number + 1)\n",
    "    writer.flush()\n",
    "\n",
    "    # Track best performance, and save the model's state\n",
    "    if avg_vloss < best_vloss:\n",
    "        best_vloss = avg_vloss\n",
    "        model_path = 'model_{}_{}'.format(timestamp, epoch_number)\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "\n",
    "    epoch_number += 1\n",
    "epoch = np.arange(len(train_accuracies))\n",
    "plt.figure()\n",
    "plt.plot(epoch, train_accuracies, 'r', epoch, valid_accuracies, 'b')\n",
    "plt.legend(['Train Accucary','Validation Accuracy'])\n",
    "plt.xlabel('Updates'), plt.ylabel('Acc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the model on test dataset\n",
    "\n",
    "For testing the model on test dataset, check `test_model.ipynb`"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d685bfd7114377445cb2c23cc6bfca7f1f2544957139cb4a27ccab868339e3e1"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
